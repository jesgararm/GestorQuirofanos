\capitulo{3}{Conceptos teóricos}

En el siguiente proyecto se hace uso de algunas técnicas de \textbf{aprendizaje automático} y \textbf{optimización} que conviene describir de cara a la mejor comprensión de los pasos seguidos hacia su resolución.

Tal y como se ha comentado, se parte de un \textbf{modelo predictivo}, basado en técnicas de \textit{aprendizaje supervisado} para estimar la duración de una intervención quirúrgica en base a los datos introducidos por el usuario.

Posteriormente, partiremos de la salida del modelo para realizar una \textbf{programación} de tipo \textit{parallel scheduling}\footnote{Programar n trabajos en m máquinas paralelas\cite{Xing2000ParallelJobs}, se asemeja a programar n intervenciones en m quirófanos simultáneos}, usando una aproximación con restricciones que definen un problema \textbf{MIP}\footnote{\textit{Mixed Integer Programming}: Problemas de programación dinámica en la que algunas de sus variables deben ser enteras.\cite{RichardsMixed-integerControl}} \cite{Lin2020AScheduling} y resultan en un problema \textit{NP-Hard}.



\section{Aprendizaje Supervisado}

El aprendizaje supervisado es un enfoque de inteligencia altificial en el cual un algoritmo es entrenado a partir de datos de entrada para producir una \textbf{determinada salida}. 

Este modelo es entrenado hasta que es capaz de detectar los patrones y relaciones entre los datos de entrada y las \textit{etiquetas} de salida, de forma que pueda otorgar resultados precisos cuando se le presenten datos \textit{nuevos}\cite{PeterssonSupervisedLearning}.

Reflejaremos en las siguientes subsecciones los diferentes algoritmos empleados para el proceso predictivo.

\subsection{Regresión Lineal}

Existen diferentes tipos de algoritmos de regresión, siendo cada uno de ellos el más adecuado en función del problema a resolver.

El mecanismo de regresión lineal más \textbf{simple} es aquel definido para dibujar una línea a través de un grafo que muestra dependencia lineal entre ambas variables y su ecuación es\cite{Belyadi2021SupervisedLearning}:
\begin{equation}
    y=mx+b
\end{equation}
Donde \textit{m} representa a la pendiente de la recta, \textit{b} a la ordenada en el origen, \textit{y} es la variable dependiente (característica de salida) y \textit{x} la independiente (característica de entrada).

Para modelos de regresión lineal múltiple, que es el que manejaremos en nuestro trabajo, usamos la ecuación siguiente\cite{Belyadi2021SupervisedLearning}: 
\begin{equation}
    y = m_{1} x_{1} + m_{2} x_{2} + ... + m_{n}x_{n} + b
\end{equation}
\subsubsection{Método de Mínimos Cuadrados}
Para obtener los parámetros de la ecuación, usaremos el \textbf{método de mínimos cuadrados}, el cual se encarga de buscar la curva que mejor se ajusta a un conjunto de puntos minimizando la suma cuadrática de los residuos a los puntos de la curva\cite{WeissteinLeastFitting}.

Operando \cite{WeissteinLeastFitting}:
\begin{equation}
    ss_{xx} = \sum_{i=1}^{n}(x_{i}-\bar{x})^2
\end{equation}
\begin{equation}
        ss_{xy} = \sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})
\end{equation}
De este modo, podemos estimar la \textit{pendiente} como:
\begin{equation}
    b = \frac{ss_{xy}}{ss_{xx}}
\end{equation}
Y la \textit{ordenada en el origen} en términos de b:
\begin{equation}
    a = \bar{y}-b\bar{x}
\end{equation}

\subsection{K-Nearest Neightbors}

Se trata de una de las formas más simples de aplicar un algoritmo de aprendizaje supervisado, con utilidad para tareas tanto de \textbf{regresión} como de \textbf{clasificación}.

Se considera un algoritmo \textit{no paramétrico} en tanto que no existen preconceptciones acerca de los datos subyacentes\cite{Belyadi2021SupervisedLearning}.

La base de su funcionamiento es la estimación de proximidad mediante el cálculo de \textbf{distancias}, en especial la \textbf{distancia euclídea}\cite{Greenacre2009CorrespondenceAnalysis}:
\begin{equation}
    d(p,q) = \sqrt{(p-q)^2}
\end{equation}

Podemos describir el algoritmo en los siguientes pasos\cite{Almomany2022OptimizedStudy}: 
\begin{enumerate}
    \item Determinar el número de vecinos más cercanos, conocido como el parámetro \textit{K}.
    \item Usar la función de \textbf{distancia} para calcular la distancia entre cada instancia del conjunto a predecir y todos los del entrenamiento.
    \item Ordena la distancia de menor a mayor y elige tantos elementos del conjunto de entrenamiento como K.
    \item Obtener la etiqueta o el valor numérico de los K-Vecinos.
    \item Devolver la \textit{media} (regresión) o la \textit{moda} (clasificación).
\end{enumerate}


\subsection{Árbol de Decisión}

Se trata de otro tipo de aprendizaje supervisado que, al igual que KNN, puede ser usado para ambos tipos de problema.

Actualmente, consiste un una de las herramientas \textbf{más usadas} para la toma de decisiones\cite{Navada2011OverviewLearning} en todo el mundo.

Para cumplir con esta tarea, se dibuja un árbol con diferentes \textit{ramas} y hojas, de forma que se incluyan \textit{todos los valores} de una situación particular\cite{Navada2011OverviewLearning}:

\imagen{Arbol de decisiones}{Ejemplo de Árbol de Decisiones}{.8}

Un árbol divide los datos en \textit{subárboles} de forma progresiva, desde la raíz, pasando por los nodos internos o de decisión y hasta llegar a los resultados, nodos hoja o \textbf{terminales}.

Existen varios algoritmos para la construcción de estos árboles, algunos de los más conocidos son ID3, C4.5, C5.0 y CART.  
De ellos, quizás el más famoso y del que derivan los demás, sea ID3, con un método de construcción \textbf{descendente y voraz} con atributos categóricos alcanzando la mayor \textit{ganancia de información}.

Conviene hacer referencia al método CART, cuya implementación optimizada por la librería \textit{scikit-learn} es la que hemos empleado en el proyecto. Este método es compatible con árboles de clasificación y regresión, aceptando tanto atributos como variables de decisión numéricas, creando árboles binarios con la mayor ganancia de información en cada nodo.\cite{Belyadi2021SupervisedLearning}

\subsubsection{Selección de Atributos}





\section{Referencias}

Las referencias se incluyen en el texto usando cite \cite{wiki:latex}. Para citar webs, artículos o libros \cite{koza92}.


\section{Imágenes}

Se pueden incluir imágenes con los comandos standard de \LaTeX, pero esta plantilla dispone de comandos propios como por ejemplo el siguiente:

\imagen{escudoInfor}{Autómata para una expresión vacía}{.5}



\section{Listas de items}

Existen tres posibilidades:

\begin{itemize}
	\item primer item.
	\item segundo item.
\end{itemize}

\begin{enumerate}
	\item primer item.
	\item segundo item.
\end{enumerate}

\begin{description}
	\item[Primer item] más información sobre el primer item.
	\item[Segundo item] más información sobre el segundo item.
\end{description}
	
\begin{itemize}
\item 
\end{itemize}

\section{Tablas}

Igualmente se pueden usar los comandos específicos de \LaTeX o bien usar alguno de los comandos de la plantilla.

\tablaSmall{Herramientas y tecnologías utilizadas en cada parte del proyecto}{l c c c c}{herramientasportipodeuso}
{ \multicolumn{1}{l}{Herramientas} & App AngularJS & API REST & BD & Memoria \\}{ 
HTML5 & X & & &\\
CSS3 & X & & &\\
BOOTSTRAP & X & & &\\
JavaScript & X & & &\\
AngularJS & X & & &\\
Bower & X & & &\\
PHP & & X & &\\
Karma + Jasmine & X & & &\\
Slim framework & & X & &\\
Idiorm & & X & &\\
Composer & & X & &\\
JSON & X & X & &\\
PhpStorm & X & X & &\\
MySQL & & & X &\\
PhpMyAdmin & & & X &\\
Git + BitBucket & X & X & X & X\\
Mik\TeX{} & & & & X\\
\TeX{}Maker & & & & X\\
Astah & & & & X\\
Balsamiq Mockups & X & & &\\
VersionOne & X & X & X & X\\
} 
